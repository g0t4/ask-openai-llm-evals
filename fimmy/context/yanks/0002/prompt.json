{"prompt":"<|repo_name|>ask-openai.nvim\n<|file_sep|>nvim-recent-yanks.txt\n## Recent yanks across all files in the project:\nChatMessage:user(content)\n\n\n{ role = \"user\", content = context.yanks.content })\n\n<|file_sep|>.ask.context\nDon't forget, any time you see a file path, in lua code... you can turn that into a require call... for example:\nlua/ask-openai/foo/bar.lua => require(\"ask-openai.foo.bar\")\n\n\n<|file_sep|>inline.lua\n<|fim_prefix|>        vim.api.nvim_buf_set_text(\n            0, -- Current buffer\n            M.selection:start_line_0indexed(), -- Zero-indexed\n            -- set start col to zero always, b/c right now only support full line\n            0, -- Zero-indexed\n            M.selection:start_line_0indexed(), -- Zero-indexed\n            -- set end line to zero always, b/c right now only support full line\n            0, -- Zero-indexed, end-exclusive column\n            lines\n        )\n\n        M.accumulated_chunks = \"\"\n    end)\nend\n\nfunction M.abort_last_request()\n    M.stop_streaming = true -- HACK to stop streaming simulations too\n\n    if not M.last_request then\n        return\n    end\n\n    backend.terminate(M.last_request)\n\n    if M.displayer ~= nil then\n        M.displayer:clear_extmarks()\n    end\nend\n\nfunction M.cleanup_after_cancel()\n    M.abort_last_request()\n    M.displayer = nil\n\n    -- PRN store this in a last_accumulated_chunks / canceled_accumulated_chunks?\n    log:info(\"Canceling this rewrite: \", M.accumulated_chunks)\n\n    M.accumulated_chunks = \"\"\nend\n\nfunction M.stream_from_ollama(user_prompt, code, file_name)\n    M.abort_last_request()\n\n    local system_prompt = \"You are a neovim AI plugin. Your name is Qwenny. \"\n        .. \"You strongly believe in the following: \"\n        .. \"1. Explanations and markdown blocks are a waste of time. No ``` nor ` surrounding your work. \"\n        .. \"2. Identation should be diligently preserved. \"\n        .. \"3. Pointless comments are infuriating. \"\n        .. \"4. Unrelated, existing code/comments must be carefully preserved (not removed, nor changed). \"\n        .. \"5. If user instructions are ambiguous, it's paramount to ask for clarification. \"\n        .. \"6. Adherence to the user's request is of utmost importance. \"\n\n    --TODO! what do I ALWAYS want for rewrites? OR NOTHING?\n    local always_include = {\n        yanks = true,\n        project = true,\n    }\n    local context = CurrentContext:items(user_prompt, always_include)\n    -- log:info(\"user_prompt: '\" .. user_prompt .. \"'\")\n    -- log:info(\"context: '\" .. vim.inspect(context) .. \"'\")\n    -- log:info(\"includes: '\" .. vim.inspect(context.includes) .. \"'\")\n\n    -- make sure to remove slash commands like /yanks (hence cleaned_prompt)\n    local user_message = context.cleaned_prompt\n    if code ~= nil and code ~= \"\" then\n        user_message = user_message\n            -- PRN move code selection to the CurrentContext type?\n            .. \"\\n Here is my code from \" .. file_name\n            .. \":\\n\" .. code\n        log:info(\"user_message: '\" .. user_message .. \"'\")\n    else\n        -- PRN detect if punctuation on end of user_message\n        user_message = user_message\n            .. \"\\n I am working on this file: \" .. file_name\n    end\n\n    local messages = {\n        { role = \"system\", content = system_prompt }\n    }\n\n    if context.includes.yanks and context.yanks then\n        table.insert(messages, C<|fim_suffix|>\n    end\n    if context.includes.commits and context.commits then\n        for _, commit in pairs(context.commits) do\n            table.insert(messages, { role = \"user\", content = commit.content })\n        end\n    end\n    if context.includes.project and context.project then\n        vim.iter(context.project)\n            :each(function(value)\n                table.insert(messages, { role = \"user\", content = value.content })\n            end)\n    end\n\n    table.insert(messages, { role = \"user\", content = user_message })\n\n    local qwen_chat_body = {\n        messages = messages,\n        --\n        -- model = \"qwen2.5-coder:7b-instruct-q8_0\",\n        model = \"qwen3:8b\", -- btw as of Qwen3, no tag == \"-instruct\", and for base you'll use \"-base\"\n        --\n        -- model = \"deepseek-r1:8b-0528-qwen3-q8_0\", -- /nothink doesn't work :(\n        --\n        -- model = \"gemma3:12b-it-q8_0\",\n        max_tokens = 8192, -- TODO set high if using /think only?\n        temperature = 0.2,\n        -- ?? do I need num_ctx (can't recall why I set it - check predicitons code)\n        -- options = {\n        --     num_ctx = 8192\n        -- }\n    }\n\n    -- local qwen_legacy_body = {\n    --     model = \"qwen2.5-coder:7b-instruct-q8_0\", -- btw -base- does terrible here :)\n    --     prompt = system_prompt .. \"\\n\" .. user_message,\n    --     temperature = 0.2,\n    -- }\n\n    -- /v1/chat/completions\n    -- local body = agentica.DeepCoder.build_chat_body(system_prompt, user_message)\n    local body = qwen_chat_body\n\n    -- /v1/completions\n    -- local body = qwen_legacy_body\n\n    -- vllm or ollama:\n    -- local base_url = \"http://build21:8000\"\n    local base_url = \"http://ollama:11434\"\n\n    M.last_request = backend.curl_for(body, base_url, M)\nend\n\nM.stop_streaming = false\nlocal function simulate_rewrite_stream_chunks(opts)\n    -- use this for timing and to test streaming diff!\n\n    M.abort_last_request()\n    vim.cmd(\"normal! 0V6jV\") -- down 5 lines from current position, 2nd v ends selection ('< and '> marks now have start/end positions)\n    vim.cmd(\"normal! 5k\") -- put cursor back before next steps (since I used 5j to move down for end of selection range\n    M.selection = Selection.get_visual_selection_for_current_window()\n    M.accumulated_chunks = \"\"\n    M.stop_streaming = false\n    M.displayer = Displayer:new(M.accept_rewrite, M.cleanup_after_cancel)\n    M.displayer:set_keymaps()\n\n    local optional_thinking_text = [[<think>\nfoo the bar lorem ipsum toodle doodle banana bie foo the bar bar the foo and foo the bar and bbbbbb the foo the bar bar the\nfoobar and foo the bar bar foo the bar lorem ipsum toodle doodle banana bie foo the bar bar the foo and foo the bar and bbbbbb\nthe foo the bar bar the foobar and foo the bar bar foo the bar lorem ipsum toodle doodle banana bie foo the bar bar the foo\nand foo the bar and bbbbbb the foo the bar bar the foobar and foo the bar bar\n</think> ]]\n    local rewritten_text = optional_thinking_text .. M.selection.original_text .. \"\\nSTREAMING w/ THINKING CONTENT\"\n    -- local rewritten_text = M.selection.original_text .. \"\\nSTREAMING NEW CONTENT\\nthis is fun\"\n\n    -- FYI can split on new line to simulate streaming lines instead of words\n    local all_words = vim.split(rewritten_text, \" \")\n\n    local fast_ms = 20\n    local slow_ms = 50<|fim_middle|>","stream":true,"options":{"num_ctx":8192},"raw":true,"num_predict":200,"model":"qwen2.5-coder:7b-base-q8_0"}