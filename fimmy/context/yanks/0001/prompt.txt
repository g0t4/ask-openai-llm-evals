<|repo_name|>ask-openai.nvim
<|file_sep|>nvim-recent-yanks.txt
## Recent yanks across all files in the project:

    if context.includes.yanks and context.yanks then
        table.insert(messages, { role = "user", content = context.yanks.content })
    end
    if context.includes.commits and context.commits then
        for _, commit in pairs(context.commits) do
            table.insert(messages, { role = "user", content = commit.content })
        end
    end
    if context.includes.project and context.project then
        vim.iter(context.project)
            :each(function(value)
                table.insert(messages, { role = "user", content = value.content })
            end)
    end


    if context.includes.yanks and context.yanks then
        table.insert(messages, { role = "user", content = context.yanks.content })
    end

<|file_sep|>.ask.context
Don't forget, any time you see a file path, in lua code... you can turn that into a require call... for example:
lua/ask-openai/foo/bar.lua => require("ask-openai.foo.bar")


<|file_sep|>ask.lua
<|fim_prefix|>local buffers = require("ask-openai.helpers.buffers")
local log = require("ask-openai.prediction.logger").predictions()
local mcp = require("ask-openai.tools.mcp")
local backend = require("ask-openai.backends.oai_chat")
local agentica = require("ask-openai.backends.models.agentica")
local ChatWindow = require("ask-openai.questions.chat_window")
local ChatThread = require("ask-openai.questions.chat_thread")
local ChatMessage = require("ask-openai.questions.chat_message")
local ChatParams = require("ask-openai.questions.chat_params")
local Selection = require("ask-openai.helpers.selection")
local CurrentContext = require("ask-openai.prediction.context")


local M = {}
require("ask-openai.helpers.buffers")


function M.send_question(user_prompt, selected_text, file_name, use_tools, entire_file)
    use_tools = use_tools or false

    M.abort_last_request()

    local system_prompt = "You are a neovim AI plugin. Your name is Qwenny. "
        .. " Please respond with markdown formatted text. Always include a TLDR at the end of your response."

    if use_tools then
        -- devstral is hesitant to use tools w/o this:
        system_prompt = system_prompt .. " You also have a set of tools you can use."
    end
    local always_include = {
        yanks = true,
        project = true,
    }
    local context = CurrentContext:items(user_prompt, always_include)

    local user_message = user_prompt
    if selected_text then
        -- would make sense to fold the code initially

        file_extension = file_name:match("%.(%w+)$") or ""

        -- TODO do not wrap in ``` block if the text has ``` in it? i.e. from markdown file
        --    I could easily drop the ``` block part, I just thought it would be nice for display in the chat history (since I use md formatting there)
        --    but AFAICT qwen does perfectly fine w/o it (I didn't have it initially and loved the responses, likely b/c I always had the file name which told the file type)

        user_message = user_message .. "\n\n"
            .. "I selected the following from `" .. file_name .. "`\n"
            .. "```" .. file_extension .. "\n"
            .. selected_text .. "\n"
            .. "```"
    end
    if entire_file then
        -- crude, take the whole file :)
        user_message = user_message .. "\n\n"
            .. "And I want you to see the entire file I am asking about:\n"
            .. "```" .. file_name .. "`\n"
            .. entire_file .. "\n"
            .. "```"
    end

    -- TODO add in other context items? toggles for these? yes! actually lets do like /foo and see if I like that

    -- show initial question
    M.chat_window:append("**system**:\n" .. system_prompt .. "\n\n**user**:\n" .. user_message)

    ---@type ChatMessage[]
    local messages = {
        ChatMessage:new("system", system_prompt),
    }
    if context.includes<|fim_suffix|>

    table.insert(messages, ChatMessage:new("user", user_message))

    ---@type ChatParams
    local qwen_params = ChatParams:new({

        -- model = "qwen2.5-coder:7b-instruct-q8_0", -- btw -base- does terrible here :)
        model = "devstral:24b-small-2505-q4_K_M",
        -- model = "devstral:24b-small-2505-q8_0",
        -- model = "qwen3:8b", -- btw as of Qwen3, no tag == "-instruct", and for base you'll use "-base"
        -- model = "gemma3:12b-it-q8_0", -- btw -base- does terrible here :)
        -- temperature = 0.2, -- TODO what temp?
        -- PRN limit num_predict?

        -- FYI - ollama, be careful w/ `num_ctx`, can't set it with OpenAI compat endpoints (whereas can pass with /api/generate)
        --   SEE NOTES about how to set this with env vars / Modelfile instead that can work with openai endpoints (don't have to use /api/generate to fix this issue)
        --   review start logs for n_ctx and during completion it warns if truncated prompt:
        --     level=WARN source=runner.go:131 msg="truncating input prompt" limit=8192 prompt=10552 keep=4 new=8192
    })
    -- /v1/chat/completions
    -- local body = agentica.DeepCoder.build_chat_body(system_prompt, user_message)
    -- PRN split agentica into messages and params

    -- ollama:
    local base_url = "http://ollama:11434"
    --
    -- vllm:
    -- local base_url = "http://build21:8000"
    -- body.model = "" -- dont pass model, use whatever is served

    if use_tools then
        -- TODO impl final test case for streaming tool_calls with vllm!
        qwen_params.tools = mcp.openai_tools()
    end

    M.thread = ChatThread:new(messages, qwen_params, base_url)
    M.send_messages()
end

function M.send_messages()
    M.hack_lines_before_request = M.chat_window.buffer:get_line_count()
    local request = backend.curl_for(M.thread:next_body(), M.thread.base_url, M)
    M.thread:set_last_request(request)
end

local function ask_question_about(opts, use_tools, include_context)
    local selection = Selection.get_visual_selection_for_current_window()
    if selection:is_empty() then
        error("No visual selection found.")
        return
    end

    local user_prompt = opts.args
    local file_name = vim.fn.expand("%:t")
    local context = include_context and buffers.get_current_buffer_entire_text() or nil

    M.ensure_response_window_is_open()
    M.send_question(user_prompt, selection.original_text, file_name, use_tools, context)
end

local function ask_question(opts, use_tools, include_context)
    local user_prompt = opts.args
    local file_name = vim.fn.expand("%:t")
    local context = include_context and buffers.get_current_buffer_entire_text() or nil

    local selection = nil
    M.ensure_response_window_is_open()
    M.send_question(user_prompt, selection, file_name, use_tools, context)
end

local function ask_question_with_context(opts)
    ask_question(opts, false, true)
end

local function ask_question_about_with_context(opts)
    ask_question_about(opts, false, true)
end

local function ask_tool_use(opts)
    ask_question(opts, true)
end

local function ask_tool_use_about(opts)
    ask_question_about(opts, true)
end


function M.abort_and_close()
    M.abort_last_request()
    if M.chat_window ~= nil then<|fim_middle|>